<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="x-ua-compatible" content="ie=edge"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width initial-scale=1"> <title>Srinivas Rao</title> <meta name="description" content="Personal website of Srinivas Rao, an undergraduate student. It enlists his publications, projects, talks, and contact information."> <meta name="robots" content="index, follow"> <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/blueimp-gallery/2.15.0/css/blueimp-gallery.min.css"> <link rel="stylesheet" href="/css/styles.css"> <link rel="canonical" href="http://localhost:4000/de/"> <noscript><style>.js-only { display: none; }</style></noscript> </head> <body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top"> <nav class="navbar navbar-light bg-faded navbar-fixed-top"> <div class="container"> <ul class="nav navbar-nav"> <li class="nav-item"> <a class="nav-link" href="#page-top"> <span class="hidden-sm-down">Srinivas Rao</span> <span class="hidden-md-up"><img src="/img/navicon.png" class="rounded" width="32" height="32" alt="top-page"></span> </a> </li> </ul> <ul class="nav navbar-nav float-xs-right" id="lang-select"> </ul> <button type="button" class="navbar-toggler hidden-sm-up float-xs-right" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button> <div class="dropdown-menu dropdown-menu-right"> <a class="dropdown-item" href="#projects">Projects</a> <a class="dropdown-item" href="#publications">Publications</a> <!-- <a class="dropdown-item" href="#talks">Talks</a> --> <a class="dropdown-item" href="#teaching">Teaching</a> <a class="dropdown-item" href="#contact">Contact</a> </div> <div class="collapse navbar-toggleable-xs"> <ul class="nav navbar-nav float-xs-right"> <li class="nav-item"><a class="nav-link" href="#projects">Projects</a></li> <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li> <!-- <li class="nav-item"><a class="nav-link" href="#talks">Talks</a></li> --> <li class="nav-item"><a class="nav-link" href="#teaching">Teaching</a></li> <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li> </ul> </div> </div> </nav> <section id="header" class="container"> <header> <div class="row flex-items-xs-center text-xs-center"> <div class="col-md-6 col-sm-8"> <img class="rounded img-fluid" width="1024" height="720" src="/img/picture.jpg" alt="portrait"> </div> <div class="col-md-8"> <h1>Hi, I&#39;m Srinivas Rao,</h1> <p class="lead">a senior undergraduate student at Indian Institute of Technology Kanpur.</p> <p class="text-xs-left">At my university I am a part of the Visual Computing Lab supervised by <a href='https://www.cse.iitk.ac.in/users/vinaypn/' style='color: rgb(0,0,225)'>Prof. Vinay Namboodiri</a>. My research interests are Computer Vision, Deep Learning and Computer Graphics (VR/AR). <br><br> Besides being an student, I also currently work part-time as a research consultant for <a href='https://fyusion.com/' style='color: rgb(0,0,225)'>Fyusion Inc</a>. (a 3D computer vision and machine learning company based in San Francisco) where I solve computer vision related problems using deep learning. A recent Techcrunch post about the company can be found <a style='color: rgb(0,0,225)' href='https://techcrunch.com/2017/12/18/fyusion-is-bringing-3d-vision-technologies-to-a-device-near-you/'>here</a>. <br><br> Last summer I was a research intern at <a href='https://fyusion.com/' style='color: rgb(0,0,225)'>Fyusion Inc</a>, San Francisco and the summer before that, I was a research intern at the wonderful <a href='https://team.inria.fr/graphdeco/team-members/' style='color: rgb(0,0,225)'>Graphdeco</a> team at Inria Sophia Antipolis, France. <br><br> Besides research, I also like to tinker with various new sensors like <a href='https://en.wikipedia.org/wiki/Tango_(platform)' style='color: rgb(0,0,225)'>Google Tango</a>, <a href='https://www.oculus.com/rift/' style='color: rgb(0,0,225)'>Oculus Rift</a>, <a href='https://www.leapmotion.com/' style='color: rgb(0,0,225)'>Leap Motion</a> in my free time. Sometimes, I try to post about some of these experiments on <a style='color: rgb(0,0,225)' href='https://www.youtube.com/channel/UCn_clMMllo53u6S8ZjF56oQ'>Youtube</a>. I believe in <a style='color: rgb(0,0,225)' href='https://en.wikipedia.org/wiki/Free_and_open-source_software'>FOSS</a> and try to write <a style='color: rgb(0,0,225)' href='https://graphics-resources.info/'>posts</a> and <a style='color: rgb(0,0,225)' href='https://www.sitepoint.com/how-to-build-an-ar-android-app-with-vuforia-and-unity/'>tutorials</a> about these experiments.</p> <p class="text-xs-left"><strong>News</strong> <br> <strong>-</strong> I will be attending Siggraph Asia 2017, Bangkok, Thailand. <br> <strong>-</strong> I will be presenting my paper at ISMAR 2017, Nantes, France. <br> <strong>-</strong> I will be interning at Fyusion Inc., San Francisco for the summer of 2017. <br> <strong>-</strong> I will be interning at Graphdeco, Inria Sophia Antipolis for the summer of 2016. </p> </div> </div> </header> </section> <section id="projects" class="container"> <!-- <div class="row text-xs-center"> <div class="col-xs"> <h1>Projects</h1> </div> </div> <div class="row"> <div class="col-lg-3 col-md-4 col-sm-6 mb-1 project"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/rnn-gan.png" alt="Thumbnail of Modeling Physics underlying visual inputs using Contextual RNN-GANs"> </div> <h5 class="mt-1">Modeling Physics underlying visual inputs using Contextual RNN-GANs</h5> <p></p> <p> <a href="https://drive.google.com/file/d/1lX_wLzywFwJjdZPpoUFzR97-_ESLSzlU/view?usp=sharing">Report</a>&nbsp;</p> </div> <div class="col-lg-3 col-md-4 col-sm-6 mb-1 project"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/voxel-flow.png" alt="Thumbnail of Novel View Synthesis"> </div> <h5 class="mt-1">Novel View Synthesis</h5> <p></p> <p> <a href="https://drive.google.com/file/d/1gM7kg4YZ8bigRYp1qxPYPqTSTJpfoemh/view?usp=sharing">Result-Sequence-1</a>&nbsp;</p> </div> <div class="col-lg-3 col-md-4 col-sm-6 mb-1 project"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/cross-modality.png" alt="Thumbnail of Cross Modality Supervision Transfer based Depth Estimation"> </div> <h5 class="mt-1">Cross Modality Supervision Transfer based Depth Estimation</h5> <p></p> <p> <a href="https://drive.google.com/file/d/0BzixkIU4HFlya2hUQmNuN0R6QVk/view?usp=sharing">Report</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyM2lCOVZhUTJJcFU/view?usp=sharing">Presentation</a>&nbsp;</p> </div> <div class="col-lg-3 col-md-4 col-sm-6 mb-1 project"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/material-edit.png" alt="Thumbnail of RGBD Material Editing using commodity depth sensors"> </div> <h5 class="mt-1">RGBD Material Editing using commodity depth sensors</h5> <p></p> <p> <a href="https://drive.google.com/file/d/1C8SN1eJ_BwDTGNvD1XrlP9_GhZsjfkud/view?usp=sharing">Presentation</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyYnpyZUltV2NXR0E/view?usp=sharing">Report-1</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlybVR0eFlQZTJkY28/view?usp=sharing">Report-2</a>&nbsp;</p> </div> <div class="col-lg-3 col-md-4 col-sm-6 mb-1 project"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/realtime-3d.png" alt="Thumbnail of Realtime 3D reconstruction based Mixed Reality"> </div> <h5 class="mt-1">Realtime 3D reconstruction based Mixed Reality</h5> <p></p> <p> <a href="https://drive.google.com/file/d/0BzixkIU4HFlybGdCeW83YWtPcWs/view?usp=sharing">Report</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyRjdlbFdRYncyalk/view?usp=sharing">Video</a>&nbsp;</p> </div> </div> --> <div class="row text-xs-center"> <div class="col-xs"> <h1>Projects</h1> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/rnn-gan.png" alt="Thumbnail of Modeling Physics underlying visual inputs using Contextual RNN-GANs"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>Modeling Physics underlying visual inputs using Contextual RNN-GANs</h5> <span class="text-muted">August to December 2017</span> <p> <a data-toggle="collapse" href="#abstract-2017-rnn-gan">Abstract</a>, <a href="https://drive.google.com/file/d/1lX_wLzywFwJjdZPpoUFzR97-_ESLSzlU/view?usp=sharing">Report</a> </p> <div class="collapse abstract" id="abstract-2017-rnn-gan"> <p>Understanding the motion of objects in order to predict and control their movements is one of the crucial problems in Artificial Intelligence (AI). It is evident that humans and a large number of animals possess this extraordinary ability of easily manipulating object motion using visual inputs. For example, it enables humans to drive vehicles safely, play games like billiards and football, navigate in crowded and newer environments. This seemingly simple, but stark, ability raises the natural question— how is the visual input used to infer and understand the motion of surrounding objects. Some recent works propose an explanatory framework based on generative physics representations, which states that the brain infers and retains a noisy, but detailed, representation of physics underlying the motion of objects and uses generative simulations based on these representations for predicting the object motion. In view of these works, it is extremely interesting and intellectually challenging to study how can such representations of physics underlying the visual inputs be modeled. Also, a model of the physical constraints on the visual inputs can be used to perform simulations, forecast object interactions and generate video sequences of moving objects. All of these potential applications are highly challenging problems in computer vision. In view of this, we aim to study the problem of modeling abstractions of physics that underlie given visual inputs and propose a contextual RNN-GAN based approach to learning these models.</p> </div> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/voxel-flow.png" alt="Thumbnail of Novel View Synthesis"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>Novel View Synthesis</h5> <p>Research Intern, Fyusion Inc., San Francisco <br><span class="text-muted">May to August 2017</span></p> <p> <a href="https://drive.google.com/file/d/1gM7kg4YZ8bigRYp1qxPYPqTSTJpfoemh/view?usp=sharing">Result-Sequence-1</a> </p> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/cross-modality.png" alt="Thumbnail of Cross Modality Supervision Transfer based Depth Estimation"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>Cross Modality Supervision Transfer based Depth Estimation</h5> <span class="text-muted">September to December 2016</span> <p> <a data-toggle="collapse" href="#abstract-2016-cross-modality">Abstract</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlya2hUQmNuN0R6QVk/view?usp=sharing">Report</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyM2lCOVZhUTJJcFU/view?usp=sharing">Presentation</a> </p> <div class="collapse abstract" id="abstract-2016-cross-modality"> <p>We propose to develop a model for depth maps estimation for an RGB sensor using the approach of supervision transfer across multiple modalities including– i. RGB and ii. Depth. The problem statement can be described as follows– We want to generate a model for depth estimation for a given RGB sensor. For this task, we consider another RGBD sensor and capture image frames that have some overlap in their field of view. We aim to learn CNN based representations for Depth of the RGB sensor using informations from the two sensors using supervision transfer across the different image modalities. We next aim to learn “invertible” CNNs to get partial network that can generate depth maps. If successful, the novel approach would add an extra modality, Depth, for the RGB sensor. The motivations for taking up this ambitious big project are twofold– i. It is an unexplored problem and has significant research value in contemporary field of vision ii. If successful, this approach will have multiple applications of immense practical importance. One such example is autonomous cars, where camera and depth sensor do not have identical field of view. It can be used in making low-cost motion capturing systems by replacing some of its many RGBD sensors with RGB cameras.</p> </div> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/material-edit.png" alt="Thumbnail of RGBD Material Editing using commodity depth sensors"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>RGBD Material Editing using commodity depth sensors</h5> <p>Research Intern, INRIA Sophia Antipolis, France (supervised by George Drettakis, Adrien Bousseau, Erik Reinhard) <br><span class="text-muted">May to July 2016</span></p> <p> <a data-toggle="collapse" href="#abstract-2016-rgbd-material">Abstract</a>, <a href="https://drive.google.com/file/d/1C8SN1eJ_BwDTGNvD1XrlP9_GhZsjfkud/view?usp=sharing">Presentation</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyYnpyZUltV2NXR0E/view?usp=sharing">Report-1</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlybVR0eFlQZTJkY28/view?usp=sharing">Report-2</a> </p> <div class="collapse abstract" id="abstract-2016-rgbd-material"> <p>We describe three approaches for texture synthesis based on material editing of indoor scenes using the data obtained from a commodity depth sensors. In the first approach we propose an extended version of patch match for material texture synthesis. The second approach is more of data- centric approach where we propose a combination of patch regression trained on a dataset and patch match based ap- proach for texture synthesis. In the third and final approach we exploit the recent advances in deep learning systems to develop an end to end pipeline for texture synthesis. The input for our system is an RGB image and a corresponding depth/normal map captured by a Kinect sensor. Material editing is carried out by changing depth/normals in a region of the image. Once editing is done, we use the proposed tex- ture synthesis algorithms to create real-like texture in the changed region of the image. We observe that the proposed approaches are able to capture structures effectively and also able to synthesize real-like textures. Each of the methods have their own limitations which are discussed in the indi- vidual sections. At the moment our approach is single view and runs offline on a PC.</p> </div> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/realtime-3d.png" alt="Thumbnail of Realtime 3D reconstruction based Mixed Reality"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>Realtime 3D reconstruction based Mixed Reality</h5> <span class="text-muted">January to April 2016</span> <p> <a data-toggle="collapse" href="#abstract-2016-realtime-3d">Abstract</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlybGdCeW83YWtPcWs/view?usp=sharing">Report</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyRjdlbFdRYncyalk/view?usp=sharing">Video</a> </p> <div class="collapse abstract" id="abstract-2016-realtime-3d"> <p>In this project, we develop a technique to create a mixed reality application where virtual objects can smartly interact with physical world. The aim of the project is to build a framework for developing various applications like helping challenged people, interactive visualizations and gaming. One of the application whose demo was shown was furniture placement in The project developed on Project Tango device [1] involves stitching of meshes acquired from the 3D point cloud data. We explore both online as well as offline techniques for stitching of meshes. Further, we augment virtual objects with animation and comprehensive interactions to forge a mixed reality and an alternate virtual reality (using a Head Mounted Display). The novelty of the proposed scheme is that all the processes except segmentation and path planning are executed real-time.</p> </div> </div> </div> </section> <section id="publications" class="container"> <div class="row text-xs-center"> <div class="col-xs"> <h1>Publications</h1> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/reactive-displays.png" alt="Thumbnail of Reactive Displays for Virtual Reality"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>Reactive Displays for Virtual Reality</h5> <p>G S S Srinivas Rao, Neeraj Thakur, and Vinay P. Namboodiri <br><span class="text-muted">Proceedings of 16th IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), Nantes, France, 2017</span></p> <p> <a data-toggle="collapse" href="#abstract-2017-reactive-displays">Abstract</a>, <a href="https://doi.org/10.1109/ISMAR-Adjunct.2017.33">DOI</a>, <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8088450">Paper</a>, <a href="https://drive.google.com/file/d/0BzixkIU4HFlyWUozTVMwUEZ2dWs/view?usp=sharing">Video</a>, <a href="https://drive.google.com/file/d/1lnfl-lhjM5y0jtIvNQeoco3AIVkVZbre/view?usp=sharing">Poster</a> </p> <div class="collapse abstract" id="abstract-2017-reactive-displays"> <p>The feeling of presence in virtual reality has enabled a large number of applications. These applications typically deal with 360° content. However, a large amount of existing content is available in terms of images and videos i.e 2D content. Unfortunately, these do not react to the viewer's position or motion when viewed through a VR HMD. Thus in this work, we propose reactive displays for VR which instigate a feeling of discovery while exploring 2D content. We create this by taking into account user's position and motion to compute homography based mappings that adapt the 2D content and re-project it onto the display. This allows the viewer to obtain a more richer experience of interacting with 2D content similar to the effect of viewing through the window at a scene. We also provide a VR interface that uses a constrained set of reactive displays to easily browse through 360° content. The proposed interface tackles the problem of nausea caused by existing interfaces like photospheres by providing a natural room-like intermediate interface before changing 360° content. We perform user studies to evaluate both of our interfaces. The results show that the proposed reactive display interfaces are indeed beneficial.</p> </div> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/spatial-audio.png" alt="Thumbnail of On the Development of a Dynamic Virtual Reality System using Audio and Visual Scenes"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>On the Development of a Dynamic Virtual Reality System using Audio and Visual Scenes</h5> <p>Sandeep Reddy, G S S Srinivas Rao, and Rajesh M Hegde <br><span class="text-muted">IEEE NCC 2016, IIT Guwhati 2016</span></p> <p> <a data-toggle="collapse" href="#abstract-2016-dynamic-vr">Abstract</a>, <a href="https://doi.org/10.1109/NCC.2016.7561204">DOI</a>, <a href="http://ieeexplore.ieee.org/document/7561204/">Paper</a> </p> <div class="collapse abstract" id="abstract-2016-dynamic-vr"> <p>Virtual reality systems have been widely used in many popular and diverse applications including education and gaming. However, development of a dynamic virtual reality system which combines both audio and visual scenes has hitherto not been investigated. In this work a dynamic virtual reality system which synchronizes both audio and visual information is developed. Realtime audio and visual information is obtained from a spherical audio visual camera with 64 microphones and 5 cameras. Subsequently, a head mounted display application is designed to render spherical video. A three dimensional sound rendering algorithm using head related transfer functions is developed. Finally, a virtual reality system that combines both spherical audio and video is realized. The head position of the user is also integrated into this system adaptively to make the system dynamic. Both subjective and objective evaluations of the proposed virtual reality system indicate its significance.</p> </div> </div> </div> <div class="row"> <div class="col-lg-2 col-sm-3"> <div class="thumbnail"> <img class="rounded img-fluid" width="480" height="640" src="/img/haof.png" alt="Thumbnail of High Accuracy Optical Flow based Future Image Predictor Model"> </div> </div> <div class="hidden-sm-up mb-1 col-xs"></div> <div class="col-lg-10 col-sm-9"> <h5>High Accuracy Optical Flow based Future Image Predictor Model</h5> <p>Nishchal K. Verma, Dhekane Eeshan Gunesh, G. S. S. Srinivas Rao, and Aakansha Mishra <br><span class="text-muted">IEEE AIPR 2015, Washington DC 2015</span></p> <p> <a data-toggle="collapse" href="#abstract-2015-optical-flow">Abstract</a>, <a href="10.1109/AIPR.2015.7444534">DOI</a>, <a href="http://ieeexplore.ieee.org/document/7444534/">Paper</a> </p> <div class="collapse abstract" id="abstract-2015-optical-flow"> <p>In this paper, High Accuracy Optical Flow (HAOF) based future image frames generator model is proposed. The aim of this work is to develop a framework which is capable of predicting the future image frames for any given sequence of images. The requirement is to predict large number of image frames with better clarity and better accuracy. In the first step, the vertical and horizontal components of flow velocities of the intensities at each pixel positions are estimated using High Accuracy Optical Flow (HAOF) algorithm. The estimated flow velocities in all the image frames at all the pixel positions are then modeled using separate Artificial Neural Networks (ANN). The trained models are used to predict the flow velocities of intensities at all the pixel positions in the future image frames. The intensities at all the pixel positions are mapped to new positions by using the velocities predicted by the model. The concept of Bilinear Interpolation is used to obtain predicted images from the new positions of intensities. The quality of the predicted image frames is evaluated by using Canny Edge Detection based Image Comparison Metric (CIM) and Mean Structural Similarity Index Measure (MSSIM). The predictor model is simulated by applying it on the two image sequences-an image sequence of a fighter jet landing over the navy deck, and another image sequence of a train moving on a bridge. The proposed framework is found to give promising results with better clarity and better accuracy.</p> </div> </div> </div> </section> <section id="talks" class="container"> <div class="row text-xs-center"> <div class="col-xs"> <h1>Talks</h1> </div> </div> <div class="row"> <div class="col-md-3 text-md-right"> 10/14/17 </div> <div class="col-md-9"> <h5>ARCore and Tango with the Unity SDK</h5> <p><span class="text-muted">Google Developer Groups, Brussels | <a href="https://www.meetup.com/gdg-brussels/events/243246115/">Event Page</a>, <a href="https://www.youtube.com/watch?v=lwEnKLTP8tY&t=1s">Video</a> </span></p> </div> </div> <div class="row mt-0"> <div class="col-md-3 text-md-right"> 06/25/16 </div> <div class="col-md-9"> <h5>Project Tango - Learn and Touch</h5> <p><span class="text-muted">Google Developer Groups, Brussels | <a href="http://vrlab-brussels.info/wiki/pub/PresentationTangoBrussels.pdf">Slides</a>, <a href="https://www.youtube.com/watch?v=BsUFb9gv1NE">Video</a> </span></p> </div> </div> <div class="row mt-0"> <div class="col-md-3 text-md-right"> 06/11/16 </div> <div class="col-md-9"> <h5>Introduction to Google&#39;s Project Tango</h5> <p><span class="text-muted">Stuttgart VR &amp; AR Meetup, Stuttgart, Germany | <a href="https://www.meetup.com/Stuttgart-VR-AR-Meetup/events/228183902/">Event Page</a>, <a href="https://drive.google.com/file/d/0B8UTMGt0lbSGbUpCamNuR25aQ0k/view?usp=sharing">Slides</a>, <a href="https://www.stuttgarter-zeitung.de/inhalt.virtuelle-realitaet-eintauchen-in-die-ueber-wirklichkeit.c66af67d-b1f5-4b50-86a8-416601085c63.html">Featured in Stuttgarter Zeitung</a> </span></p> </div> </div> <div class="row mt-0"> <div class="col-md-3 text-md-right"> 05/18/16 </div> <div class="col-md-9"> <h5>Introduction to Google&#39;s Project Tango</h5> <p><span class="text-muted">Google Developers Group, Nice, France | <a href="https://www.meetup.com/GDGNice/events/230346062/">Event Page</a> </span></p> </div> </div> </section> <section id="teaching" class="container"> <!-- <div class="row text-xs-center"> <div class="col-xs"> <h1>Teaching</h1> </div> </div> <div class="row flex-items-xs-center"> <div class="col-md-5 text-md-right text-xs-left col-sm-10"> <h5>2017 : Summer Term</h5> </div> <div class="col-md-5 col-sm-10"> <h5>2016/17 : Winter Term</h5> <p>Advanced Programming in C++<br><span class="text-muted">Lecture (MA), Tutor &amp; Co-Lecturer</span></p> <p>Methods and Techniques of Information Visualization<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Real-time Monitoring of Massive File Systems<br><span class="text-muted">Project (MA), Supervisor</span></p> </div> </div> <div class="row flex-items-xs-center"> <div class="col-md-5 text-md-right text-xs-left col-sm-10"> <h5>2016 : Summer Term</h5> <p>Computergraphics II<br><span class="text-muted">Lecture (BA), Tutor &amp; Co-Lecturer</span></p> <p>Methods and Techniques of Software Visualization<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Introduction to Visual Analytics<br><span class="text-muted">Seminar (BA), Tutor</span></p> <p>Massive Information Mining for Software Analytics<br><span class="text-muted">Project (BA), Supervisor</span></p> </div> <div class="col-md-5 col-sm-10"> <h5>2015/16 : Winter Term</h5> <p>Systems Engineering for Software Analytics<br><span class="text-muted">Seminar (MA), Tutor</span></p> </div> </div> <div class="row flex-items-xs-center"> <div class="col-md-5 text-md-right text-xs-left col-sm-10"> <h5>2015 : Summer Term</h5> <p>Automated Visual Software Analytics<br><span class="text-muted">Lecture (massive open online course), Tutor</span></p> <p>Visualization of System Evolution<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Programming Technique II<br><span class="text-muted">Lecture (BA), Tutor &amp; Co-Lecturer</span></p> <p>Software Analysis and Visualization<br><span class="text-muted">Project (BA), Supervisor</span></p> </div> <div class="col-md-5 col-sm-10"> <h5>2014/15 : Winter Term</h5> <p>Visualization for Interactive Software Analytics<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Introduction to Programming C++<br><span class="text-muted">Lecture (MA Geo), Lecturer</span></p> <p>Game Programming<br><span class="text-muted">Lecture/Seminar (BA), Tutor &amp; Co-Lecturer</span></p> <p>Introduction to 3D Computergraphics<br><span class="text-muted">Youth Academy, Lecturer</span></p> </div> </div> <div class="row flex-items-xs-center"> <div class="col-md-5 text-md-right text-xs-left col-sm-10"> <h5>2014 : Summer Term</h5> <p>Information Visualization<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Graphics Programming with OpenGL and C++<br><span class="text-muted">Lecture/Seminar (BA), Tutor &amp; Lecturer</span></p> <p>Software Analysis and Visualization<br><span class="text-muted">Project (BA), Supervisor</span></p> </div> <div class="col-md-5 col-sm-10"> <h5>2013/14 : Winter Term</h5> <p>Software Analytics<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Game Programming<br><span class="text-muted">Lecture/Seminar (BA), Tutor &amp; Co-Lecturer</span></p> <p>Computergraphics II<br><span class="text-muted">Lecture (BA), Tutor &amp; Co-Lecturer</span></p> <p>Introduction to 3D Computergraphics<br><span class="text-muted">Youth Academy, Lecturer</span></p> </div> </div> <div class="row flex-items-xs-center"> <div class="col-md-5 text-md-right text-xs-left col-sm-10"> <h5>2013 : Summer Term</h5> <p>Software Visualization Techniques<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Graphics Programming with OpenGL and C++<br><span class="text-muted">Lecture/Seminar (BA), Tutor &amp; Lecturer</span></p> <p>Computergraphics I<br><span class="text-muted">Lecture (BA), Tutor &amp; Co-Lecturer</span></p> </div> <div class="col-md-5 col-sm-10"> <h5>2012/13 : Winter Term</h5> <p>Concepts and Techniques for 3D Visualization<br><span class="text-muted">Seminar (MA), Tutor</span></p> <p>Game Programming<br><span class="text-muted">Lecture/Seminar (BA), Tutor &amp; Co-Lecturer</span></p> <p>Computergraphics II<br><span class="text-muted">Lecture (BA), Tutor &amp; Co-Lecturer</span></p> <p>Introduction to 3D Computergraphics<br><span class="text-muted">Youth Academy, Lecturer</span></p> </div> </div> --> </section> <section id="contact" class="container"> <div class="row text-xs-center mb-1"> <div class="col-xs"> <h1>Contact</h1> </div> </div> <form class="mt-1" id="contact-form" action="//formspree.io/sairao1996@gmail.com" method="POST"> <div class="form-group row mb-0"> <label class="col-sm-3 col-form-label text-sm-right text-xs-left">E-Mail:</label> <div class="col-sm-9"> <p class="form-control-static mb-0"><a href="mailto:sairao1996@gmail.com">sairao1996@gmail.com</a></p> <p class="form-control-static text-muted"> Other: gsssrao@iitk.ac.in, srao@fyusion.com </p> </div> </div> <div class="form-group row mb-0"> <label class="col-sm-3 col-form-label text-sm-right text-xs-left">More:</label> <div class="col-sm-9"> <p class="form-control-static"> <a href="https://github.com/gsssrao">GitHub</a>, <a href="https://plus.google.com/u/0/107028785825750725408">Google+</a>, <a href="https://flipboard.com/@gsssrao">Flipboard</a>, <a href="https://www.youtube.com/channel/UCn_clMMllo53u6S8ZjF56oQ">YouTube</a> </p> </div> </div> </form> </section> <section id="footer"> <footer> <div class="container"> <div class="row flex-items-xs-center text-xs-center"> <div class="col-md-8 col-xs"> <small>This website was based on the template by <a href="https://github.com/cgcostume/cgcostume.github.io">Daniel Limberger</a> and is hosted via <a href="https://pages.github.com/">GitHub Pages</a> for free. Last updated on <a href="https://github.com/cgcostume/cgcostume.github.io/commits/master">03/23/18</a>.</small> </div> </div> </div> </footer> <div id="blueimp-gallery" class="blueimp-gallery js-only"> <div class="slides">#</div> <h3 class="title">#</h3> <a class="prev">‹</a> <a class="next">›</a> <a class="close">×</a> <a class="play-pause"></a> <ol class="indicator"></ol> </div> </section> <script src="https://cdn.jsdelivr.net/g/jquery@3.1.1,tether@1.2.0,jquery.validation@1.16.0,clipboard.js@1.5.16,blueimp-gallery@2.15.0,bootstrap@4.0.0-alpha.5"></script> <script src="/js/datafolio.js"></script> </body> </html>
